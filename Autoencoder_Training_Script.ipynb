{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tier1Security/roberta_classifier/blob/main/Autoencoder_Training_Script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "from transformers import RobertaModel, RobertaTokenizer\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# ==========================================\n",
        "# 1. Model Definition (RoBERTa + Decoder)\n",
        "# ==========================================\n",
        "class RobertaAutoencoder(nn.Module):\n",
        "    def __init__(self, model_name=\"roberta-base\"):\n",
        "        super(RobertaAutoencoder, self).__init__()\n",
        "        # Load your fine-tuned encoder weights here\n",
        "        self.encoder = RobertaModel.from_pretrained(model_name)\n",
        "\n",
        "        # The Decoder: Attempts to reconstruct the 768-dim embedding\n",
        "        # Malicious commands will cause high Reconstruction Loss (MSE)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(768, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(1024, 768)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Get the hidden state (CLS token)\n",
        "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        embedding = outputs.last_hidden_state[:, 0, :] # Shape: [batch, 768]\n",
        "\n",
        "        # Attempt reconstruction\n",
        "        reconstructed = self.decoder(embedding)\n",
        "        return embedding, reconstructed\n",
        "\n",
        "# ==========================================\n",
        "# 2. Dataset Handler\n",
        "# ==========================================\n",
        "class BaselineDataset(Dataset):\n",
        "    def __init__(self, csv_file, tokenizer, max_len=128):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.data.iloc[idx]['command'])\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten()\n",
        "        }\n",
        "\n",
        "# ==========================================\n",
        "# 3. Training Loop\n",
        "# ==========================================\n",
        "def train_anomaly_engine(csv_path, model_save_path=\"anomaly_engine.pt\"):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "    model = RobertaAutoencoder().to(device)\n",
        "\n",
        "    dataset = BaselineDataset(csv_path, tokenizer)\n",
        "    loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "    criterion = nn.MSELoss() # Loss between original and reconstructed embedding\n",
        "\n",
        "    print(f\"[*] Training Anomaly Engine on {len(dataset)} benign samples...\")\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(3): # Usually converges quickly on benign data\n",
        "        total_loss = 0\n",
        "        for batch in tqdm(loader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            mask = batch['attention_mask'].to(device)\n",
        "\n",
        "            emb, rec = model(input_ids, mask)\n",
        "\n",
        "            # Loss is the 'struggle' to understand the command\n",
        "            loss = criterion(rec, emb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1} Loss: {total_loss/len(loader)}\")\n",
        "\n",
        "    # Calculate Anomaly Threshold (Z-Score Baseline)\n",
        "    model.eval()\n",
        "    errors = []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            mask = batch['attention_mask'].to(device)\n",
        "            emb, rec = model(input_ids, mask)\n",
        "            # Row-wise MSE\n",
        "            error = torch.mean((emb - rec)**2, dim=1)\n",
        "            errors.extend(error.cpu().numpy())\n",
        "\n",
        "    threshold = np.mean(errors) + (3 * np.std(errors))\n",
        "    print(f\"[+] Training Complete. Anomaly Threshold set to: {threshold}\")\n",
        "\n",
        "    torch.save({\n",
        "        'model_state': model.state_dict(),\n",
        "        'threshold': threshold\n",
        "    }, model_save_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Ensure you have the benign_baseline.csv generated first\n",
        "    # train_anomaly_engine(\"benign_baseline.csv\")\n",
        "    pass"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "wuMQqliimhxX"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}